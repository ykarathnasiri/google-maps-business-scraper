{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install selenium pandas openpyxl webdriver-manager requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zu6gar51GI8U",
        "outputId": "eb60e3ab-8166-4ab3-a944-ab25cf9a79ce"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.35.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Collecting webdriver-manager\n",
            "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: urllib3<3.0,>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
            "Collecting trio~=0.30.0 (from selenium)\n",
            "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.12.2 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2025.6.15 in /usr/local/lib/python3.12/dist-packages (from selenium) (2025.8.3)\n",
            "Collecting typing_extensions~=4.14.0 (from selenium)\n",
            "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from webdriver-manager) (1.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from webdriver-manager) (25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Collecting outcome (from trio~=0.30.0->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.12.2->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
            "Downloading selenium-4.35.0-py3-none-any.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
            "Downloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, typing_extensions, outcome, webdriver-manager, trio, trio-websocket, selenium\n",
            "  Attempting uninstall: typing_extensions\n",
            "    Found existing installation: typing_extensions 4.15.0\n",
            "    Uninstalling typing_extensions-4.15.0:\n",
            "      Successfully uninstalled typing_extensions-4.15.0\n",
            "Successfully installed outcome-1.3.0.post0 selenium-4.35.0 trio-0.30.0 trio-websocket-0.12.2 typing_extensions-4.14.1 webdriver-manager-4.0.2 wsproto-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium.common.exceptions import TimeoutException\n",
        "import urllib.parse\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "# ===== CONFIGURATION SECTION =====\n",
        "CONFIG = {\n",
        "    \"district\": \"Colombo\",\n",
        "    \"city\": \"Kotte\",\n",
        "    \"country\": \"Sri Lanka\",\n",
        "    \"industry\": \"gyms\",\n",
        "    \"max_results\": 10,\n",
        "    \"output_file\": None  # will be set dynamically\n",
        "}\n",
        "\n",
        "def setup_driver():\n",
        "    print(\"Setting up Chrome WebDriver...\")\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument('--headless')\n",
        "    chrome_options.add_argument('--no-sandbox')\n",
        "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "    chrome_options.add_argument('--disable-gpu')\n",
        "    chrome_options.add_argument('--disable-software-rasterizer')\n",
        "    chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
        "    chrome_options.add_argument('--window-size=1920,1080')\n",
        "    chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
        "    chrome_options.add_experimental_option('useAutomationExtension', False)\n",
        "\n",
        "    try:\n",
        "        service = Service(ChromeDriverManager().install())\n",
        "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "    except:\n",
        "        chrome_options.add_argument('--remote-debugging-port=9222')\n",
        "        driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "    driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
        "    print(\"WebDriver setup completed\")\n",
        "    return driver\n",
        "\n",
        "def force_full_maps(driver):\n",
        "    print(\"Checking for full Google Maps version...\")\n",
        "    try:\n",
        "        if \"lite\" in driver.current_url or \"preview\" in driver.current_url:\n",
        "            print(\"Lite mode detected, forcing full version...\")\n",
        "            try:\n",
        "                larger_map_link = driver.find_element(By.PARTIAL_LINK_TEXT, \"View larger map\")\n",
        "                larger_map_link.click()\n",
        "                time.sleep(5)\n",
        "                print(\"Switched to full version\")\n",
        "                return True\n",
        "            except:\n",
        "                current_url = driver.current_url\n",
        "                if \"lite\" in current_url:\n",
        "                    full_url = current_url.replace(\"/lite/\", \"/\").replace(\"lite.\", \"\")\n",
        "                    driver.get(full_url)\n",
        "                    time.sleep(5)\n",
        "                    print(\"URL modified to full version\")\n",
        "                    return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def search_on_google_maps(driver, district, city, country, industry):\n",
        "    search_query = f\"{industry} in {city}, {district}, {country}\"\n",
        "    print(f\"Searching for: {search_query}\")\n",
        "    encoded_query = urllib.parse.quote(search_query)\n",
        "    maps_url = f\"https://www.google.com/maps/search/{encoded_query}\"\n",
        "\n",
        "    try:\n",
        "        driver.get(maps_url)\n",
        "        print(\"Loading Google Maps...\")\n",
        "        time.sleep(10)\n",
        "        force_full_maps(driver)\n",
        "\n",
        "        result_selectors = [\n",
        "            '//div[@role=\"feed\"]',\n",
        "            '//div[@id=\"pane\"]',\n",
        "            '//div[contains(@class, \"Nv2PK\")]'\n",
        "        ]\n",
        "\n",
        "        wait = WebDriverWait(driver, 30)\n",
        "        print(\"Looking for search results...\")\n",
        "        for selector in result_selectors:\n",
        "            try:\n",
        "                wait.until(EC.presence_of_element_located((By.XPATH, selector)))\n",
        "                print(\"Search results found\")\n",
        "                return True\n",
        "            except TimeoutException:\n",
        "                continue\n",
        "        print(\"No search results found\")\n",
        "        return False\n",
        "    except:\n",
        "        print(\"Error during search\")\n",
        "        return False\n",
        "\n",
        "def scroll_and_load_results(driver):\n",
        "    print(\"Scrolling to load all results...\")\n",
        "    try:\n",
        "        scrollable_element = driver.find_element(By.XPATH, '//div[@role=\"feed\"]')\n",
        "    except:\n",
        "        print(\"No scrollable element found, skipping scroll\")\n",
        "        return True\n",
        "\n",
        "    last_height = driver.execute_script(\"return arguments[0].scrollHeight\", scrollable_element)\n",
        "    scroll_count = 0\n",
        "    for i in range(20):\n",
        "        driver.execute_script(\"arguments[0].scrollTop = arguments[0].scrollHeight\", scrollable_element)\n",
        "        time.sleep(3)\n",
        "        new_height = driver.execute_script(\"return arguments[0].scrollHeight\", scrollable_element)\n",
        "        scroll_count += 1\n",
        "        print(f\"Scroll {scroll_count}/20 - Loading more results...\")\n",
        "        if new_height == last_height:\n",
        "            print(\"No more results to load\")\n",
        "            break\n",
        "        last_height = new_height\n",
        "    print(\"Scrolling completed\")\n",
        "    return True\n",
        "\n",
        "def get_business_links(driver):\n",
        "    print(\"Extracting business links...\")\n",
        "    business_links = []\n",
        "    elements = driver.find_elements(By.XPATH, '//a[contains(@href, \"/maps/place/\")]')\n",
        "    for element in elements:\n",
        "        href = element.get_attribute('href')\n",
        "        if href and '/maps/place/' in href and href not in business_links:\n",
        "            business_links.append(href)\n",
        "    unique_links = list(set(business_links))[:CONFIG['max_results']]\n",
        "    print(f\"Found {len(unique_links)} business links\")\n",
        "    return unique_links\n",
        "\n",
        "def extract_business_details(driver, business_url):\n",
        "    details = {\n",
        "        'Name': '',\n",
        "        'Phone': '',\n",
        "        'Rating': '',\n",
        "        'Review_Count': '',\n",
        "        'Address': '',\n",
        "        'Website': '',\n",
        "        'Google_Maps_Link': business_url\n",
        "    }\n",
        "    try:\n",
        "        driver.get(business_url)\n",
        "        time.sleep(5)\n",
        "\n",
        "        # Name\n",
        "        try:\n",
        "            name_selectors = [\n",
        "                '//h1[@class=\"DUwDvf lfPIob\"]',\n",
        "                '//h1[contains(@class, \"fontHeadlineLarge\")]',\n",
        "                '//div[contains(@class, \"fontHeadlineLarge\")]',\n",
        "                '//h1',\n",
        "                '//span[contains(@class, \"DUwDvf\")]'\n",
        "            ]\n",
        "            for selector in name_selectors:\n",
        "                try:\n",
        "                    details['Name'] = driver.find_element(By.XPATH, selector).text.strip()\n",
        "                    if details['Name']:\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Rating and Review Count\n",
        "        try:\n",
        "            rating_selectors = [\n",
        "                '//span[@class=\"MW4etd\"]',\n",
        "                '//div[contains(@class, \"F7nice\")]//span[contains(@aria-label, \"stars\")]',\n",
        "                '//span[contains(@aria-label, \"stars\")]'\n",
        "            ]\n",
        "\n",
        "            for selector in rating_selectors:\n",
        "                try:\n",
        "                    rating_element = driver.find_element(By.XPATH, selector)\n",
        "                    rating_text = rating_element.text or rating_element.get_attribute('aria-label')\n",
        "\n",
        "                    # Extract rating\n",
        "                    rating_match = re.search(r'(\\d+\\.?\\d*)', rating_text)\n",
        "                    if rating_match:\n",
        "                        details['Rating'] = rating_match.group(1)\n",
        "\n",
        "                    # Try to find review count in nearby elements\n",
        "                    try:\n",
        "                        # Look for review count in parent or sibling elements\n",
        "                        parent = rating_element.find_element(By.XPATH, './..')\n",
        "                        parent_text = parent.text\n",
        "\n",
        "                        # Multiple patterns for review count\n",
        "                        review_patterns = [\n",
        "                            r'(\\d{1,3}(?:,\\d{3})*)\\s*(?:reviews?|Reviews?)',\n",
        "                            r'(\\d+)\\s*(?:reviews?|Reviews?)',\n",
        "                            r'\\((\\d{1,3}(?:,\\d{3})*)\\)',\n",
        "                            r'Based on (\\d{1,3}(?:,\\d{3})*)'\n",
        "                        ]\n",
        "\n",
        "                        for pattern in review_patterns:\n",
        "                            review_match = re.search(pattern, parent_text)\n",
        "                            if review_match:\n",
        "                                details['Review_Count'] = review_match.group(1).replace(',', '')\n",
        "                                break\n",
        "\n",
        "                        # If not found in parent, try grandparent\n",
        "                        if not details['Review_Count']:\n",
        "                            try:\n",
        "                                grandparent = parent.find_element(By.XPATH, './..')\n",
        "                                grandparent_text = grandparent.text\n",
        "                                for pattern in review_patterns:\n",
        "                                    review_match = re.search(pattern, grandparent_text)\n",
        "                                    if review_match:\n",
        "                                        details['Review_Count'] = review_match.group(1).replace(',', '')\n",
        "                                        break\n",
        "                            except:\n",
        "                                pass\n",
        "\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                    if details['Rating']:\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Alternative review count search\n",
        "        if not details['Review_Count']:\n",
        "            try:\n",
        "                review_selectors = [\n",
        "                    '//span[contains(text(), \"reviews\")]',\n",
        "                    '//span[contains(text(), \"Reviews\")]',\n",
        "                    '//button[contains(@aria-label, \"reviews\")]',\n",
        "                    '//*[contains(text(), \"reviews\") or contains(text(), \"Reviews\")]'\n",
        "                ]\n",
        "\n",
        "                for selector in review_selectors:\n",
        "                    try:\n",
        "                        elements = driver.find_elements(By.XPATH, selector)\n",
        "                        for element in elements:\n",
        "                            text = element.text or element.get_attribute('aria-label')\n",
        "                            if text:\n",
        "                                review_match = re.search(r'(\\d{1,3}(?:,\\d{3})*)\\s*(?:reviews?|Reviews?)', text)\n",
        "                                if review_match:\n",
        "                                    details['Review_Count'] = review_match.group(1).replace(',', '')\n",
        "                                    break\n",
        "                        if details['Review_Count']:\n",
        "                            break\n",
        "                    except:\n",
        "                        continue\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Phone\n",
        "        try:\n",
        "            phone_selectors = [\n",
        "                '//button[contains(@data-item-id, \"phone\")]//div[contains(@class, \"Io6YTe\")]',\n",
        "                '//span[contains(@aria-label, \"Phone\")]',\n",
        "                '//a[contains(@href, \"tel:\")]'\n",
        "            ]\n",
        "            for selector in phone_selectors:\n",
        "                try:\n",
        "                    phone_element = driver.find_element(By.XPATH, selector)\n",
        "                    phone_text = phone_element.text or phone_element.get_attribute('href')\n",
        "                    if phone_text:\n",
        "                        details['Phone'] = phone_text.replace('tel:', '').strip()\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Address\n",
        "        try:\n",
        "            address_selectors = [\n",
        "                '//button[@data-item-id=\"address\"]//div[contains(@class, \"Io6YTe\")]',\n",
        "                '//span[contains(@aria-label, \"Address\")]',\n",
        "                '//div[contains(@class, \"rogA2c\")]//div[contains(@class, \"Io6YTe\")]'\n",
        "            ]\n",
        "            for selector in address_selectors:\n",
        "                try:\n",
        "                    address_element = driver.find_element(By.XPATH, selector)\n",
        "                    details['Address'] = address_element.text.strip()\n",
        "                    if details['Address']:\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Website\n",
        "        try:\n",
        "            website_selectors = [\n",
        "                '//a[@data-item-id=\"authority\"]//div[contains(@class, \"Io6YTe\")]',\n",
        "                '//a[contains(@href, \"http\") and not(contains(@href, \"google\"))]'\n",
        "            ]\n",
        "            for selector in website_selectors:\n",
        "                try:\n",
        "                    website_element = driver.find_element(By.XPATH, selector)\n",
        "                    website = website_element.get_attribute('href') or website_element.text\n",
        "                    if website and 'google' not in website.lower():\n",
        "                        details['Website'] = website.strip()\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        if details['Name']:\n",
        "            print(f\"Extracted: {details['Name']} (Rating: {details['Rating']}, Reviews: {details['Review_Count']})\")\n",
        "            return details\n",
        "        else:\n",
        "            print(\"Failed to extract business name\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting business details: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def save_to_excel(business_data_list, district, city, industry):\n",
        "    if not business_data_list:\n",
        "        print(\"No data to save\")\n",
        "        return\n",
        "    df = pd.DataFrame(business_data_list)\n",
        "    output_file = f\"{industry}_{city}.xlsx\".replace(\" \", \"_\")\n",
        "    sheet_name = f\"{industry}_{district}_{city}\".replace(\" \", \"_\")[:31]\n",
        "    print(f\"Saving data to {output_file}...\")\n",
        "    with pd.ExcelWriter(output_file, engine='openpyxl', mode='w') as writer:\n",
        "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
        "    print(f\"Data saved to {output_file} ({len(df)} records)\")\n",
        "\n",
        "def main():\n",
        "    print(f\"Starting scraping process for {CONFIG['industry']} in {CONFIG['city']}, {CONFIG['district']}\")\n",
        "    print(f\"Target: {CONFIG['max_results']} results\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    driver = None\n",
        "    all_business_data = []\n",
        "    try:\n",
        "        driver = setup_driver()\n",
        "        if not search_on_google_maps(driver, CONFIG['district'], CONFIG['city'], CONFIG['country'], CONFIG['industry']):\n",
        "            print(\"Search failed, exiting...\")\n",
        "            return\n",
        "\n",
        "        scroll_and_load_results(driver)\n",
        "        business_links = get_business_links(driver)\n",
        "\n",
        "        if not business_links:\n",
        "            print(\"No business links found\")\n",
        "            return\n",
        "\n",
        "        print(f\"Processing {len(business_links)} businesses:\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        for i, business_url in enumerate(business_links, 1):\n",
        "            print(f\"[{i}/{len(business_links)}] Processing business...\")\n",
        "            business_details = extract_business_details(driver, business_url)\n",
        "            if business_details:\n",
        "                all_business_data.append(business_details)\n",
        "                print(f\"Successfully extracted data for {business_details['Name']}\")\n",
        "            else:\n",
        "                print(\"Failed to extract business data\")\n",
        "            time.sleep(random.uniform(2, 4))\n",
        "\n",
        "        print(\"-\" * 50)\n",
        "        if all_business_data:\n",
        "            save_to_excel(all_business_data, CONFIG['district'], CONFIG['city'], CONFIG['industry'])\n",
        "            print(f\"Scraping completed successfully! Total records: {len(all_business_data)}\")\n",
        "        else:\n",
        "            print(\"No valid business data extracted\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {str(e)}\")\n",
        "    finally:\n",
        "        if driver:\n",
        "            driver.quit()\n",
        "            print(\"Browser closed\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lCKWsWkUrdfo",
        "outputId": "6ac05e40-b727-410c-d7e4-b6abebcd38f7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting scraping process for gyms in Kotte, Colombo\n",
            "Target: 10 results\n",
            "--------------------------------------------------\n",
            "Setting up Chrome WebDriver...\n",
            "WebDriver setup completed\n",
            "Searching for: gyms in Kotte, Colombo, Sri Lanka\n",
            "Loading Google Maps...\n",
            "Checking for full Google Maps version...\n",
            "Looking for search results...\n",
            "Search results found\n",
            "Scrolling to load all results...\n",
            "Scroll 1/20 - Loading more results...\n",
            "Scroll 2/20 - Loading more results...\n",
            "Scroll 3/20 - Loading more results...\n",
            "Scroll 4/20 - Loading more results...\n",
            "Scroll 5/20 - Loading more results...\n",
            "Scroll 6/20 - Loading more results...\n",
            "Scroll 7/20 - Loading more results...\n",
            "Scroll 8/20 - Loading more results...\n",
            "Scroll 9/20 - Loading more results...\n",
            "Scroll 10/20 - Loading more results...\n",
            "Scroll 11/20 - Loading more results...\n",
            "Scroll 12/20 - Loading more results...\n",
            "Scroll 13/20 - Loading more results...\n",
            "Scroll 14/20 - Loading more results...\n",
            "Scroll 15/20 - Loading more results...\n",
            "Scroll 16/20 - Loading more results...\n",
            "No more results to load\n",
            "Scrolling completed\n",
            "Extracting business links...\n",
            "Found 10 business links\n",
            "Processing 10 businesses:\n",
            "--------------------------------------------------\n",
            "[1/10] Processing business...\n",
            "Extracted: Sparta (Pvt) Ltd (Rating: 4.7, Reviews: 50)\n",
            "Successfully extracted data for Sparta (Pvt) Ltd\n",
            "[2/10] Processing business...\n",
            "Extracted: Fitness 360 (Rating: 4.7, Reviews: 64)\n",
            "Successfully extracted data for Fitness 360\n",
            "[3/10] Processing business...\n",
            "Extracted: Super Fitness Center (Rating: 4.4, Reviews: 145)\n",
            "Successfully extracted data for Super Fitness Center\n",
            "[4/10] Processing business...\n",
            "Extracted: Mantra Life (Rating: 5.0, Reviews: 3)\n",
            "Successfully extracted data for Mantra Life\n",
            "[5/10] Processing business...\n",
            "Extracted: Fitness House Personal Training Studio (Rating: 4.4, Reviews: 94)\n",
            "Successfully extracted data for Fitness House Personal Training Studio\n",
            "[6/10] Processing business...\n",
            "Extracted: Alpha Lee Fitness (Rating: 5.0, Reviews: 6)\n",
            "Successfully extracted data for Alpha Lee Fitness\n",
            "[7/10] Processing business...\n",
            "Extracted: MDRTGYM (Rating: 5.0, Reviews: 4)\n",
            "Successfully extracted data for MDRTGYM\n",
            "[8/10] Processing business...\n",
            "Extracted: BBK boho fitness (Rating: 4.4, Reviews: 356)\n",
            "Successfully extracted data for BBK boho fitness\n",
            "[9/10] Processing business...\n",
            "Extracted: Fitfinity Fitness (Rating: 4.5, Reviews: 114)\n",
            "Successfully extracted data for Fitfinity Fitness\n",
            "[10/10] Processing business...\n",
            "Extracted: Power World Gym - Ethul Kotte (Rating: 4.1, Reviews: 168)\n",
            "Successfully extracted data for Power World Gym - Ethul Kotte\n",
            "--------------------------------------------------\n",
            "Saving data to gyms_Kotte.xlsx...\n",
            "Data saved to gyms_Kotte.xlsx (10 records)\n",
            "Scraping completed successfully! Total records: 10\n",
            "Browser closed\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}